# Q 和 K 的致命危机（点积爆炸）：
在 Attention 的核心数学公式里，Q 和 K 是要做**点积（Dot Product）**的：
$$\text{Score} = \frac{Q \cdot K^T}{\sqrt{d}}$$
随后，这个 Score 会被送进 $\text{Softmax}$ 激活函数。
问题来了：在大规模训练或者超长文本推理时，Q 和 K 里的浮点数容易变得特别大。
一旦两个很大的数相乘，Score 就会爆炸。送进 Softmax 后，会导致一个极其尖锐的概率分布（比如一个是 1.0，其他全是 0.0000001）。
这在学术界叫“注意力熵崩溃”（Attention Entropy Collapse），模型会瞬间变智障，丧失对上下文的综合关注能力。
Qwen 的解法（Q-K Norm）：
既然 Q 和 K 相乘容易爆炸，那就在它们相乘之前，强行用 attn_q_norm 和 attn_k_norm 把它们俩压回标准的正态分布：$$Q' = \text{RMSNorm}(Q, \text{attn\_q\_norm.weight})$$$$K' = \text{RMSNorm}(K, \text{attn\_k\_norm.weight})$$
这样一来，无论文本有多长，无论模型跑了多久，$Q' \cdot K'^T$ 的点积结果永远被死死按在一个极其安全的数值范围内。

**需要注意**，在 Transformer 中，每个 Head 是一个独立的语义维度（比如有的 Head 关注语法，有的关注逻辑）。如果对整个 4096 归一化： 强力的 Head 会“吃掉”弱小 Head 的方差，导致不同 Head 之间的信息相互干扰。

**按 Head 归一化（Per-head Norm）**： 确保每一个 Head 内部的 128 个神经元都被强行拉回到一个稳定的数学区间。这样在做后续的点积计算 $Q_{head} \cdot K_{head}^T$ 时，每一个 Head 的注意力分布都能保持独立且稳定的敏感度。

# Feed-Forward Network，前馈网络
在完成 Attention 车间（负责理解上下文）的计算后，数据会进入 Block 的第二个核心车间：FFN（Feed-Forward Network，前馈网络）。

如果说 Attention 是在做“选择题”（决定看哪个词），那么 FFN 就是在做“问答题”（提取和加工知识）。你列出的这四个张量，共同构成了这个知识加工流水线。

### 1. 进入 FFN 前的“安检”
#### ffn_norm:
- 作用：它是 FFN 模块的 Pre-Normalization 权重。
- 物理意义：数据在经过 Attention 层的残差累加后，数值分布可能又变得不稳定了。在进入复杂的非线性变换（FFN）之前，必须用 ffn_norm 再次进行 RMSNorm 归一化，确保输入到下一个矩阵的数据是“标准且安全”的。

### 2. 知识加工的“三叉戟”：Gate, Up, Down
Qwen 和 Llama 架构使用的都是 SwiGLU 激活函数，这是一种比传统神经网络更复杂、但效果更好的设计。它不再是简单的“输入 -> 隐藏层 -> 输出”，而是分成两条路并行加工。

#### ffn_up：空间的“拉升器”
- 形状：[4096, 12288] (对应你的日志：embedding_length 到 feed_forward_length)。
- 作用：它把 4096 维的数据投影到 12288 维的高维空间。
- 直觉理解：这就像把一张折叠的纸展开，让模型有更多的“维度”去拆解当前词汇包含的深层含义。

#### ffn_gate：知识的“过滤器”
- 形状：也是 [4096, 12288]。
- 作用：它与 ffn_up 并行计算。它的输出会经过一个叫做 SiLU (Sigmoid Linear Unit) 的激活函数。
- 数学公式：Output = (SiLU(ffn_gate(x)) * ffn_up(x))。
- 物理意义：gate 的作用是决定 up 提取出的万千信息中，哪些是真正对当前语境有用的。它像一个开关，根据当前的语义筛选出需要被保留的知识。

#### ffn_down：维度的“压缩机”
- 形状：[12288, 4096]。
- 作用：在 12288 维空间完成筛选和加工后，数据必须被压回原始的 4096 维，以便能加回到主干公路上（残差连接）。
- 物理意义：它是知识加工的最后一步，将提炼后的高维洞察总结成模型能继续传递的信号。


## TransientState
### 1. 主动脉 (The Residual Stream)
* **`x: []f32` (大小: `dim`, 即 4096)**
  * **职责**：这是整个模型中**最核心的残差流（Residual Stream）向量**。
  * **生命周期**：它就是你刚刚用 Embedding Lookup 提取出来的那 4096 个浮点数。在接下来的几十层 Transformer Block 中，所有的计算结果都会通过加法不断累加到这个 `x` 上（即公式 $x = x + \text{Attention}(x)$ 和 $x = x + \text{FFN}(x)$）。经过所有层后，它携带了对下一个词的全部预测信息。



### 2. 归一化与中转站 (Normalization Buffers)
* **`xb: []f32` (大小: `dim`, 4096)**
  * **职责**：存放 `x` 经过 **RMSNorm（均方根归一化）** 之后的结果。
  * **为什么需要它**：因为我们不能直接覆盖 `x`（后续残差相加还要用到原始的 `x`）。你刚才写的 `rmsnorm` 函数，正是把 `x` 读出来，稳压处理后，写入了 `xb` 中。接下来计算 Q、K、V 时，矩阵乘法的输入就是这个 `xb`。

* **`xb2: []f32` (大小: `dim`, 4096)**
  * **职责**：另一个临时中转站。在计算完 Attention 或 FFN 后，最终要加回残差流 `x` 之前，通常会将投影矩阵（Output Projection）的结果暂存在这里。



### 3. 注意力机制核心 (Attention Mechanism)
* **`q: []f32` (大小: `dim`, 4096)**
  * **职责**：当前 Token 的 **Query（查询）** 向量。由 `xb` 与权重矩阵 $W_q$ 做矩阵乘法得到。它代表“当前这个词正在寻找什么上下文信息”。

* **`k: []f32` (大小: `kv_dim`, 例如 1024)**
  * **职责**：当前 Token 的 **Key（键）** 向量。由 `xb` 与权重 $W_k$ 相乘得到。它代表“当前这个词包含什么特征”。*(注意：Qwen 使用分组查询注意力 GQA，所以 K 的维度通常比 Q 小，以节省显存)*。

* **`v: []f32` (大小: `kv_dim`, 例如 1024)**
  * **职责**：当前 Token 的 **Value（值）** 向量。由 `xb` 与权重 $W_v$ 相乘得到。它代表“如果别人匹配到了我，我应该把什么实际内容传递过去”。

* **`att: []f32` (大小: `head_count * max_seq_len`)**
  * **职责**：存放 **注意力分数（Attention Scores）**。在做点积注意力时，我们将当前的 `q` 与历史缓存中所有的 `k` 进行点积 ($Q \cdot K^T$)，得到的分数存放在这里，并就地执行 Softmax 函数转化为 0~1 的概率权重。

### 4. 前馈神经网络 (Feed-Forward Network / SwiGLU)
* **`hb: []f32` (大小: `hidden_dim`, 例如 12288)**
* **`hb2: []f32` (大小: `hidden_dim`, 例如 12288)**
  * **职责**：这是 FFN 层专用的**高维膨胀空间 (Hidden Buffers)**。
  * **数学逻辑**：现代 LLM（如 Qwen/Llama）使用 SwiGLU 激活函数。它需要两条并行的路径：
    1. `hb` 存放门控输出：$hb = \text{SiLU}(xb \cdot W_{gate})$
    2. `hb2` 存放线性输出：$hb2 = xb \cdot W_{up}$
    3. 然后两者做逐元素的标量乘法：$hb = hb \otimes hb2$。之后再乘以 $W_{down}$ 矩阵降维回 4096 维。

### 5. 最终决策层 (The Output)
* **`logits: []f32` (大小: `vocab_size`, 即 151936)**
  * **职责**：存放对全词表 15 万个 Token 的**原始打分（Logits）**。
  * **生命周期**：当 `x` 穿过了所有的 Transformer 层，它会经历最后一次 RMSNorm，然后与模型最后的分类器矩阵（`output.weight`）相乘。产生的结果填满这个数组，找出里面数值最大的那个 index，就是模型预测的下一个词。

这就是所有内存空间的物理意义。既然你的 RMSNorm 已经算出了 `xb`，下一步就是拿着 `xb` 和第 0 层的 $W_q$、$W_k$、$W_v$ 权重做**矩阵乘法**，算出 `q`、`k`、`v` 这三个关键向量了。需要我直接给出这个最高频算子（`matvec`）的实现方案吗？
